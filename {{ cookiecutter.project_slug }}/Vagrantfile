# -*- mode: ruby -*-
# vi: set ft=ruby :

ENV["VAGRANT_EXPERIMENTAL"] = "typed_triggers"

# Set volume path using create_args instead of docker.volumes, workaround for
# issue when running Docker on WSL 2, see
# https://github.com/hashicorp/vagrant/issues/12602
DEFAULT_DOCKER_CREATE_ARGS = [
  '--mount', 'type=tmpfs,destination=/run',
  '--mount', 'type=tmpfs,destination=/run/lock',
  '-v', '/sys/fs/cgroup:/sys/fs/cgroup:rw',
  '--cap-add=NET_ADMIN',
  '--cgroupns=host'
]

Vagrant.require_version ">= 2.2.5"

Vagrant.configure("2") do |config|

  config.vagrant.plugins = {
    "vagrant-proxyconf" => {"version" => ">=2.0.10"},
    "vagrant-vbguest" => {"version" => ">=0.24.0"},
    "vagrant-bindfs" => {"version" => ">=1.1.9"}
  }

  # Prefer using the VirtualBox provider over Docker if both are available
  # https://www.vagrantup.com/docs/providers/basic_usage#default-provider
  config.vm.provider "virtualbox"
  config.vm.provider "docker"

  # If we're using VirtualBox as the provider, apply the following config for all VMs
  config.vm.provider :virtualbox do |virtualbox, override|
    override.vm.box = "ubuntu/jammy64"
    override.vbguest.auto_update = false
    # There seems to be a VirtualBox bug where the VirtualBox processes on the host machine
    # can get stuck at 100% after resuming from sleep on macOS hosts, even when the guest
    # machines are idle. We disable audio devices on all VMs as a workaround.
    # See https://www.virtualbox.org/ticket/18089
    virtualbox.customize ["modifyvm", :id, "--audio", "none"]
  end

  # If we're using Docker as the provider, apply the following config for all containers
  config.vm.provider :docker do |docker, override|
    docker.image = "{{ cookiecutter.project_slug}}_base_image"
    docker.remains_running = true
    docker.has_ssh = true
    # Set volume path using create_args instead of docker.volumes,
    # workaround for issue when running Docker on WSL 2, see
    # https://github.com/hashicorp/vagrant/issues/12602
    docker.create_args = [
      '--mount', 'type=tmpfs,destination=/run',
      '--mount', 'type=tmpfs,destination=/run/lock',
      '-v', '/sys/fs/cgroup:/sys/fs/cgroup:rw',
      '--cap-add=NET_ADMIN',
      '--cgroupns=host'
    ]
  end

  config.apt_proxy.http = "{{ cookiecutter.vagrant_apt_proxy }}"
  config.proxy.http = "{{ cookiecutter.vagrant_http_proxy }}"
  config.proxy.https = "{{ cookiecutter.vagrant_https_proxy }}"
  config.proxy.enabled = {% if cookiecutter.vagrant_apt_proxy or cookiecutter.vagrant_http_proxy or cookiecutter.vagrant_https_proxy %}true{% else %}false{% endif %}

  config.vm.define "backend", primary: true do |backend|
    backend.vm.network "private_network", ip: "192.168.56.2"
    # Forward the https port being served via nginx
    backend.vm.network "forwarded_port", guest: 443, host: 4000
    # Forward the a port that can be used when running the django dev server
    backend.vm.network "forwarded_port", guest: 4001, host: 4001
    # Enables X11 forwarding. This can be useful when running Selenium tests
    # since you can see exactly what's happening in the browser during the tests.
    # This requires an X11 server on your host machine, see https://www.xquartz.org
    # for Mac hosts or http://x.cygwin.com for Windows hosts.
    backend.ssh.forward_x11 = true
    backend.vm.synced_folder(
      "./backend_app",
      "/opt/{{ cookiecutter.project_slug }}_backend/current",
    )

    backend.vm.provider :virtualbox do |virtualbox, override|
      virtualbox.memory = 1024
      virtualbox.cpus = 1
      # Configure a bindfs mount for the virtualenv 'venv' directory,
      # preventing venv files from being shared within the VirtualBox shared folder.
      # This avoids the performance hit of running a venv from the slow shared folder.
      override.trigger.after :"Vagrant::Action::Builtin::WaitForCommunicator", type: :action do |trigger|
        trigger.run_remote = {inline: "mkdir -p /home/vagrant/backend_app_venv"}
      end
      override.bindfs.bind_folder(
        "/home/vagrant/backend_app_venv",
        "/opt/{{ cookiecutter.project_slug }}_backend/current/venv",
        after: :synced_folders
      )
    end

    backend.vm.provider :docker do |docker, override|
      docker.create_args = DEFAULT_DOCKER_CREATE_ARGS + [
        "-v", "{{ cookiecutter.project_slug }}_venv:/opt/{{ cookiecutter.project_slug }}_backend/current/venv"
      ]
      override.trigger.after :"Vagrant::Action::Builtin::WaitForCommunicator", type: :action do |trigger|
        trigger.run_remote = {inline: "chown vagrant:vagrant /opt/{{ cookiecutter.project_slug }}_backend/current/venv"}
      end
    end

  end

  config.vm.define "frontend" do |frontend|
    frontend.vm.network "private_network", ip: "192.168.56.3"
    # Forward the https port being served via nginx
    frontend.vm.network "forwarded_port", guest: 443, host: 5000

    frontend.vm.provider :virtualbox do |virtualbox|
      virtualbox.memory = 512
    end
  end

  config.vm.define "db" do |db|
    db.vm.network "private_network", ip: "192.168.56.4"

    db.vm.provider :virtualbox do |virtualbox|
      virtualbox.memory = 512
    end
  end

  # We spin up another VM and execute ansible using the ansible_local vagrant
  # provisioner, as opposed to executing ansible directly from the host machine.
  #
  # This helps simplify the initial setup process, creates a consistent
  # provisioning environment and allows us to provision a dev environment on
  # host machines where Ansible is not supported, such as Windows.
  # (see https://docs.ansible.com/ansible/latest/user_guide/windows_faq.html#can-ansible-run-on-windows )
  #
  # However, this process does result in the creation of a 'provisioner' VM,
  # which increases the system resources used for this dev environment.
  config.vm.define "provisioner" do |provisioner|
    provisioner.vm.network "private_network", ip: "192.168.56.5"
    # Forward a port that can be used if starting the frontend dev server
    provisioner.vm.network "forwarded_port", guest: 5001, host: 5001

    # Mount the folder containing the dev vm ssh keys with explicit file
    # permissions. This addresses 'unprotected private key file' errors on
    # Windows, due to mounts on Windows having 777 file permissions by default.
    provisioner.vm.synced_folder(
      "./.vagrant/machines",
      "/vagrant_machines",
      mount_options: ["fmode=600"]
    )

    provisioner.vm.provider :virtualbox do |virtualbox, override|
      virtualbox.memory = 1024
      # Configure a bindfs mount for the large 'node_modules' directory,
      # preventing these files from being shared within the VirtualBox shared folder.
      # This avoids the performance hit of accessing a large number of files via
      # the slow VirtualBox shared folder
      override.trigger.after :"Vagrant::Action::Builtin::WaitForCommunicator", type: :action do |trigger|
        trigger.run_remote = {inline: "mkdir -p /home/vagrant/frontend_app_node_modules"}
      end
      override.bindfs.bind_folder(
        "/home/vagrant/frontend_app_node_modules",
        "/vagrant/frontend_app/node_modules",
        after: :synced_folders
      )
    end

    provisioner.vm.provider :docker do |docker, override|
      docker.create_args = DEFAULT_DOCKER_CREATE_ARGS + [
        "-v", "{{ cookiecutter.project_slug }}_node_modules:/vagrant/frontend_app/node_modules"
      ]
      override.trigger.after :"Vagrant::Action::Builtin::WaitForCommunicator", type: :action do |trigger|
        trigger.run_remote = {inline: "chown vagrant:vagrant /vagrant/frontend_app/node_modules"}
      end
    end

    provisioner.vm.provision "file" do |file|
      file.source = File.join(
        File.dirname(__FILE__), "provisioning/dev_ssh_config"
      )
      file.destination = "/home/vagrant/.ssh/config"
    end

    provisioner.vm.provision "shell" do |shell|
      shell.name = "ansible setup"
      shell.privileged = false
      # install ansible requirements
      shell.inline = <<-SCRIPT
        set -e
        sudo apt-get update
        sudo apt-get install -y curl rsync python3 python3-venv python3-pip python3-apt libssl-dev
        python3.10 -m venv ~/provisioning_venv --system-site-packages
        source ~/provisioning_venv/bin/activate
        pip install pip==22.3.1 wheel==0.38.4
        pip install -r /vagrant/provisioning/requirements.txt
      SCRIPT
    end

    # See https://www.vagrantup.com/docs/provisioning/ansible_local.html and
    # https://www.vagrantup.com/docs/provisioning/ansible_common.html
    provisioner.vm.provision "ansible_local" do |ansible|
      ansible.compatibility_mode = "2.0"
      ansible.playbook = "playbook.yml"
      ansible.playbook_command = "~/provisioning_venv/bin/ansible-playbook"
      ansible.provisioning_path = "/vagrant/provisioning"
      ansible.config_file = "ansible_vagrant.cfg"
      ansible.install = false # we do this in the above inline setup script
      ansible.galaxy_role_file = "requirements.yml"
      ansible.galaxy_command = " ~/provisioning_venv/bin/ansible-galaxy install --role-file=%{role_file}"
      ansible.verbose = false
      ansible.limit = "all,localhost"
      ansible.inventory_path = "environments/dev/inventory"
      ansible.tags = ENV["VAGRANT_ANSIBLE_TAGS"] || "all"
    end
  end

end
